# configs/trainer/default.yaml

init:
  _target_: pytorch_lightning.Trainer
  max_epochs: 1000
  accelerator: "auto"
  devices: 1
  precision: "16-mixed"
  gradient_clip_val: 1.0
  gradient_clip_algorithm: "norm"
  accumulate_grad_batches: 1
  log_every_n_steps: 25
  check_val_every_n_epoch: 1
  enable_checkpointing: true
  enable_progress_bar: true
  enable_model_summary: true
  deterministic: false
  benchmark: true
  
  
  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      monitor: "val/loss"
      mode: "min"
      save_top_k: 1         
      save_last: false      
      filename: "best"
      auto_insert_metric_name: false
      
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      save_top_k: 0         
      save_last: true       
      filename: "last"      
      verbose: false
      
    # - _target_: pytorch_lightning.callbacks.EarlyStopping
    #   monitor: "val/loss"
    #   patience: 20
    #   mode: "min" 
      
    - _target_: pytorch_lightning.callbacks.LearningRateMonitor
      logging_interval: "epoch"

    # - _target_: optuna_integration.pytorch_lightning.PyTorchLightningPruningCallback
    #   monitor: "val/loss"


