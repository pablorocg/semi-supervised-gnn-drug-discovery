# configs/baseline_config.yaml

defaults:
  - dataset: moleculenet  # Options: qm9, moleculenet, ogbg
  - model: ginev2  # Options: gin, gcn, gnn, gine
  - lightning_module: mean_teacher
  - trainer: default
  - logger: wandb
  - _self_  

# General settings
seed: 42
force_deterministic: true

# Override configuration for specific components
# This is how it looks up the dataset config file
# configs/lightning_module/mean_teacher.yaml
name: "mean_teacher"

init:
  _target_: src.lightning_modules.mean_teacher.MeanTeacherModule
  learning_rate: 5e-5
  warmup_epochs: 75
  cosine_period_ratio: 0.5
  compile: false
  weights: null
  optimizer: "adamw"
  weight_decay: 1e-3
  nesterov: true
  momentum: 0.99
  
  # Mean Teacher specific hyperparameters
  ema_decay: 0.999  # Try: [0.99, 0.995, 0.999]
  consistency_rampup_epochs: 10  # Try: [5, 10, 20]
  max_consistency_weight: 0.5  # Try: [0.1, 1.0, 5.0, 10.0]
  
  validate_features: true

    # Experiments:
    # [0.72, 0.08, 0.1, 0.1] x 
    # [0.65, 0.15, 0.1, 0.1] x 
    # [0.50, 0.30, 0.1, 0.1] x 
    # [0.30, 0.50, 0.1, 0.1]

# Current overrides applied by the user:
dataset:
  init:
    splits: [0.35, 0.35, 0.2, 0.1] # This overrides the splits from the above block
    mode: "semisupervised" # This overrides the mode from the above block
    batch_size_train: 16 # This overrides the batch_size_train from the above block


# Hydra configuration 
hydra:
  run:
    dir: ${oc.env:LOGS_DIR}/hydra/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${oc.env:LOGS_DIR}/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}